{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from nltk import *\n",
    "from stop_words import get_stop_words\n",
    "\n",
    "# import string\n",
    "from sklearn import feature_extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class spanish_tweet_tokenizer_with_date:\n",
    "    \"\"\"Recibe ts o df con dos columnas: fecha en formato twitter (como índice) y el texto de los tweets\"\"\"\n",
    "    \"\"\"Receives ts or df with two columns: date in twitter format (as index) and tweets text.\"\"\"\n",
    "    def __init__(self,tweets):\n",
    "        # Asumo que la información no tiene estorbos (nans, cosas distintas a fechas en el índice...)\n",
    "        # I assume info hasn't any hindrance\n",
    "        try:\n",
    "            tweets.stack()\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            date = pd.to_datetime([time.strftime('%Y-%m-%d %H:%M:%S', j) \n",
    "                                   for j in [time.strptime(i,'%a %b %d %H:%M:%S +0000 %Y') for i in tweets.index]])\n",
    "        except:\n",
    "            date = pd.to_datetime(tweets.index)\n",
    "        tokenizer = tokenize.simple.SpaceTokenizer() #para evitar problemas con acentuación\n",
    "        self.date_corrected = pd.Series(tweets.values,\n",
    "                                        index=date)\n",
    "        \n",
    "        tweets = tweets.str.lower()\n",
    "        tokens = []\n",
    "        dates  = []\n",
    "        tknd_tweets = [] #aquí almacenaré tweets ya tokenizados\n",
    "        try:\n",
    "            decoded_tweets = [x.decode('utf-8') for x in tweets.astype('string')]\n",
    "        except IndexError:\n",
    "            decoded_tweets = [x.decode('utf-8') for x in tweets.iloc[:,1].astype('string')]\n",
    "        for t in xrange(len(decoded_tweets)):\n",
    "            tokened = tokenizer.tokenize(decoded_tweets[t])\n",
    "            tknd_tweets.append(tokened)\n",
    "            tweet_date = date[t]\n",
    "            for word in xrange(len(tokened)):\n",
    "                dates.append(tweet_date)\n",
    "                if len(tokened[word])<=1:\n",
    "                    tokens.append(tokened[word])\n",
    "                elif tokened[word][-1] in [sign.decode('utf-8') for sign in ['?','!',',',':','.','-','\"',')']]:\n",
    "                    tokens.append(tokened[word][:-1])\n",
    "                elif tokened[word][0] in [sign.decode('utf-8') for sign in ['¿','¡',',',':','.','-','\"', '(']]:\n",
    "                    tokens.append(tokened[word][1:])\n",
    "                else:\n",
    "                    tokens.append(tokened[word].lower())\n",
    "        frame = pd.DataFrame(np.asarray(tokens).reshape(len(tokens)),\n",
    "                             columns=['tokens'],\n",
    "                             index=pd.to_datetime(dates))\n",
    "        self.tokened_tweets_df = frame\n",
    "        self.tokened_tweets_ts = frame.stack()\n",
    "        self.tokened_tweets = frame.values\n",
    "        self.n_tokens = frame.shape[0]\n",
    "        self.n_tweets = tweets.shape[0]\n",
    "        \n",
    "        # Agregaré en automático el cómputo sin stop_words\n",
    "        stop_words_sp = get_stop_words('spanish')\n",
    "        #algunas palabras que no incluye\n",
    "        stop_words_sp.append(' ')\n",
    "        stop_words_sp.append('')\n",
    "        stop_words_sp.append('-')\n",
    "        stop_words_sp.append(u'si')\n",
    "        stop_words_sp.append(u'sí')\n",
    "        ix_no_sw    = np.in1d(self.tokened_tweets, stop_words_sp)!=1 #stopwords\n",
    "        frame_no_sw = self.tokened_tweets_ts[ix_no_sw]\n",
    "\n",
    "        self.tokened_tweets_no_sw = frame_no_sw\n",
    "        self.n_tokens_no_sw       = frame_no_sw.shape[0]\n",
    "    \n",
    "    def n_tokens(self):\n",
    "        return self.n_words\n",
    "    \n",
    "    def n_tweets(self):\n",
    "        return self.n_tweets\n",
    "    \n",
    "    def lexical_diversity(self):\n",
    "        distinct_words = len(set([word for word in self.tokened_tweets_ts]))\n",
    "        total_words    = self.tokened_tweets.shape[0]\n",
    "        \n",
    "        lex_div = 1.*distinct_words/total_words\n",
    "        return lex_div\n",
    "    \n",
    "    def avg_words_p_tweet(self):\n",
    "        avg_words = 1.*self.n_tokens/self.n_tweets\n",
    "        return avg_words\n",
    "    \n",
    "    def tokened_tweets_no_sw(self):\n",
    "        return self.tokened_tweets_no_sw\n",
    "        \n",
    "    def n_tokens_no_sw(self):\n",
    "        return self.n_tokens_no_sw\n",
    "    \n",
    "    def lexical_diversity_no_sw(self):\n",
    "        distinct_words = len(set([word for word in self.tokened_tweets_no_sw]))\n",
    "        total_words    = self.n_tokens_no_sw\n",
    "\n",
    "        lex_div = 1.*distinct_words/total_words\n",
    "        return lex_div\n",
    "\n",
    "    def avg_words_p_tweet_no_sw(self):\n",
    "        avg_words = 1.*self.n_tokens_no_sw/self.n_tweets\n",
    "        return avg_words\n",
    "    \n",
    "    def lookfor(self, words_to_look, include_sw):\n",
    "        \"\"\"Returns a filtered ts that includes only words_to_look, over ts with or without stopwords\"\"\"\n",
    "        if include_sw==True:\n",
    "            frame = self.tokened_tweets_no_sw\n",
    "            n_obs = self.n_tokens_no_sw\n",
    "        else:\n",
    "            frame = self.tokened_tweets_ts\n",
    "            n_obs = self.n_tokens\n",
    "        \n",
    "        temp_ix = np.ones((1,n_obs))\n",
    "        \n",
    "        for w in words_to_look:\n",
    "            temp_ix = np.concatenate((temp_ix,frame.str.match(w).values.reshape(1,n_obs)),\n",
    "                                     axis=0)\n",
    "        ix    = pd.DataFrame(temp_ix[1:,:])\n",
    "        \n",
    "        final = frame.loc[(ix.mean()>0).values]\n",
    "        \n",
    "        return final\n",
    "    def lookfor_intweets(self, words_to_look):\n",
    "        \"\"\"Retrieves tweets that contain any of the given words.\"\"\"\n",
    "        frame = self.date_corrected.str.lower()\n",
    "        n_obs = frame.shape[0]\n",
    "        \n",
    "        temp_ix = np.ones((1,n_obs))\n",
    "        \n",
    "        for w in words_to_look:\n",
    "            temp_ix = np.concatenate((temp_ix,frame.str.contains(w).values.reshape(1,n_obs)),\n",
    "                                     axis=0)\n",
    "        ix    = pd.DataFrame(temp_ix[1:,:])\n",
    "        final = frame.loc[(ix.sum()>0).values]\n",
    "        \n",
    "        return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dict  = {0:'banamex',\n",
    "              1:'SantanderMx',\n",
    "              2:'ScotiabankMX',\n",
    "              3:'BBVABancomer'}\n",
    "n_accounts = len(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Palabras promocionales y de exp al cliente\n",
    "#Creo que la forma más eficiente es identificar la fecha e identificar ese tweet\n",
    "capta   = [u'vista',u'plazo',u'interés',u'intereses',u'tasa',\n",
    "           u'débito',u'debito',u'depósito',u'deposit',u'cuenta']\n",
    "\n",
    "credit  = [u'crédito',u'credit', u'hipoteca',u'auto',u'nómina',u'personal',\n",
    "           u'tarjeta',u'tdc',u'arrendamiento',u'msi']\n",
    "\n",
    "invest  = [u'inversi',u'pagaré',u'dinero',u'bolsa',\n",
    "           u'invierte',u'fondo',u'ahorr',u'invertir']\n",
    "\n",
    "tdc     = [u'tdc',u'puntos',u'cashback',u'meses',u'balance',\n",
    "           u'saldo',u'transfer',u'cash',u'msi',u'tarjeta',\n",
    "           u'compras',u'paga',u'pagos',]\n",
    "\n",
    "exp_clt = [u'sucursal',u'servicio',u'atención',u'atend',u'queja']\n",
    "\n",
    "seguro  = [u'seguro',u'inmobili',u'médico',u'gastos',u'vida']\n",
    "\n",
    "general = [u'comision',u'comisión',u'promo',u'descuento',\n",
    "           u'regístrate',u'conoce',u'tyc', u'términos', \n",
    "           u'terminos', u'condici',u'vívelo',u'experienc',\n",
    "           u'programa',u'vive',u'contrat',u'adquiere',\n",
    "           u'aprovecha',u'vigencia',u'deuda',u'operaci',\n",
    "           u'liquida',u'gratis',u'gratui',u'premia',u'express',\n",
    "           u'compras',u'ventas',u'ingresa',u'especial',u'membresía',\n",
    "           u'empresarial']\n",
    "\n",
    "viajes  = [u'viaj',u'destino',u'vuela',u'planea',\n",
    "           u'anticipación',u'avión']\n",
    "\n",
    "shows   = [u'boleto',u'preven',u'concierto',u'exclusiva',\n",
    "           u'moto',u'vehículo',u'vehicular',u'cine',u'ticket'\n",
    "           u'asiste',u'experiencia']\n",
    "\n",
    "autos   = [u'carro',u'coche','auto']\n",
    "\n",
    "casa    = [u'casa',u'hogar',u'inmobiliario',u'vivienda']\n",
    "\n",
    "mobile  = [u'internet',u'móvil',u'banca',u'servicio',\n",
    "           u'celular', u'localizador',u'telefónic',\n",
    "           u'descarga',u'línea',u'sms',u'app',u'wallet',\n",
    "           u'mobile',u'consulta',u'laptop',u'sms',u'internet',\n",
    "           u'retiro',u'cargo']\n",
    "\n",
    "mkt_words = capta+credit+invest+tdc+exp_clt+seguro+general+viajes+shows+autos+casa+mobile\n",
    "\n",
    "\n",
    "divisas  = [u'dólar', u'peso', u'dolar',u'indicador',\n",
    "            u'tipo de cambio',u'tipodecambio']\n",
    "vacantes = [u'vacante',u'linkedin',u'trabajo']\n",
    "talleres = [u'taller',u'finanzas']\n",
    "\n",
    "inf_words = divisas+vacantes\n",
    "\n",
    "other_news = [u'sabíasque',u'cultura']\n",
    "\n",
    "\n",
    "clt_words = [u'hola', u'favor', u'contacto', u'saludos', u'nombre', \n",
    "             u'dm', u'report', u'sentimos', u'=', u'llamada', u'agrade',\n",
    "             u'gracias', u'caso', u'atendemos',u'lamenta',u'molestia',\n",
    "             u'buenas',u'buenos',u'md']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "mkt_categos     = {u'Captación':capta, \n",
    "                   u'Crédito':credit,\n",
    "                   u'Inversiones':invest, \n",
    "                   'Experiencia al cliente':exp_clt,\n",
    "                   'TdC':tdc, \n",
    "                   'Seguro':seguro, \n",
    "                   'General':general, \n",
    "                   'Viajes':viajes, \n",
    "                   'Shows':shows, \n",
    "                   'Autos':autos, \n",
    "                   'Casa':casa, \n",
    "                   u'Móvil o Internet':mobile}\n",
    "\n",
    "mkt_categos_tags = [u'Captación',\n",
    "                    u'Crédito',\n",
    "                    u'Inversiones',\n",
    "                    u'TdC',\n",
    "                    'Experiencia al cliente',\n",
    "                    'Seguro', \n",
    "                    'General', \n",
    "                    'Viajes', \n",
    "                    'Shows', \n",
    "                    'Autos', \n",
    "                    'Casa', \n",
    "                    u'Móvil o Internet'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "banamex\n",
      "(3201, 24)\n",
      "(3201, 18)\n",
      "(6400L,)\n",
      "(3601L,)\n",
      "Captación\n",
      "----- 13 (13L,)\n",
      "(12L,)\n",
      "Crédito\n",
      "----- 54 (54L,)\n",
      "(50L,)\n",
      "Inversiones\n",
      "----- 10 (10L,)\n",
      "(10L,)\n",
      "TdC\n",
      "----- 97 (97L,)\n",
      "(91L,)\n",
      "Experiencia al cliente\n",
      "----- 6 (6L,)\n",
      "(6L,)\n",
      "Seguro\n",
      "----- 18 (18L,)\n",
      "(17L,)\n",
      "General\n",
      "----- 206 (206L,)\n",
      "(190L,)\n",
      "Viajes\n",
      "----- 14 (14L,)\n",
      "(14L,)\n",
      "Shows\n",
      "----- 98 (98L,)\n",
      "(85L,)\n",
      "Autos\n",
      "----- 5 (5L,)\n",
      "(5L,)\n",
      "Casa\n",
      "----- 4 (4L,)\n",
      "(3L,)\n",
      "Móvil o Internet\n",
      "----- 86 (86L,)\n",
      "(83L,)\n",
      "\n",
      "SantanderMx\n",
      "(3201, 24)\n",
      "(3201, 18)\n",
      "(6400L,)\n",
      "(6396L,)\n",
      "Captación\n",
      "----- 523 (523L,)\n",
      "(108L,)\n",
      "Crédito\n",
      "----- 374 (374L,)\n",
      "(193L,)\n",
      "Inversiones\n",
      "----- 40 (40L,)\n",
      "(25L,)\n",
      "TdC\n",
      "----- 361 (361L,)\n",
      "(202L,)\n",
      "Experiencia al cliente\n",
      "----- 911 (911L,)\n",
      "(103L,)\n",
      "Seguro\n",
      "----- 51 (51L,)\n",
      "(35L,)\n",
      "General\n",
      "----- 361 (361L,)\n",
      "(184L,)\n",
      "Viajes\n",
      "----- 50 (50L,)\n",
      "(37L,)\n",
      "Shows\n",
      "----- 85 (85L,)\n",
      "(65L,)\n",
      "Autos\n",
      "----- 59 (59L,)\n",
      "(36L,)\n",
      "Casa\n",
      "----- 50 (50L,)\n",
      "(41L,)\n",
      "Móvil o Internet\n",
      "----- 305 (305L,)\n",
      "(82L,)\n",
      "\n",
      "ScotiabankMX\n",
      "(3200, 24)\n",
      "(3199, 18)\n",
      "(6397L,)\n",
      "(4524L,)\n",
      "Captación\n",
      "----- 106 (106L,)\n",
      "(50L,)\n",
      "Crédito\n",
      "----- 177 (177L,)\n",
      "(126L,)\n",
      "Inversiones\n",
      "----- 19 (19L,)\n",
      "(12L,)\n",
      "TdC\n",
      "----- 149 (149L,)\n",
      "(88L,)\n",
      "Experiencia al cliente\n",
      "----- 364 (364L,)\n",
      "(75L,)\n",
      "Seguro\n",
      "----- 40 (40L,)\n",
      "(27L,)\n",
      "General\n",
      "----- 322 (322L,)\n",
      "(177L,)\n",
      "Viajes\n",
      "----- 15 (15L,)\n",
      "(12L,)\n",
      "Shows\n",
      "----- 95 (95L,)\n",
      "(83L,)\n",
      "Autos\n",
      "----- 60 (60L,)\n",
      "(53L,)\n",
      "Casa\n",
      "----- 12 (12L,)\n",
      "(5L,)\n",
      "Móvil o Internet\n",
      "----- 325 (325L,)\n",
      "(83L,)\n",
      "\n",
      "BBVABancomer\n",
      "(3200, 24)\n",
      "(3200, 18)\n",
      "(6398L,)\n",
      "(3614L,)\n",
      "Captación\n",
      "----- 49 (49L,)\n",
      "(43L,)\n",
      "Crédito\n",
      "----- 65 (65L,)\n",
      "(62L,)\n",
      "Inversiones\n",
      "----- 127 (127L,)\n",
      "(108L,)\n",
      "TdC\n",
      "----- 163 (163L,)\n",
      "(155L,)\n",
      "Experiencia al cliente\n",
      "----- 31 (31L,)\n",
      "(30L,)\n",
      "Seguro\n",
      "----- 80 (80L,)\n",
      "(74L,)\n",
      "General\n",
      "----- 275 (275L,)\n",
      "(259L,)\n",
      "Viajes\n",
      "----- 18 (18L,)\n",
      "(18L,)\n",
      "Shows\n",
      "----- 17 (17L,)\n",
      "(17L,)\n",
      "Autos\n",
      "----- 12 (12L,)\n",
      "(11L,)\n",
      "Casa\n",
      "----- 11 (11L,)\n",
      "(11L,)\n",
      "Móvil o Internet\n",
      "----- 171 (171L,)\n",
      "(154L,)\n"
     ]
    }
   ],
   "source": [
    "# Cómo se publica de acuerdo a cada subcategoría de mkt (desde enero)\n",
    "for b in xrange(n_accounts):\n",
    "    print '\\n',data_dict[b]\n",
    "    \n",
    "    datum1 = pd.read_csv('tweets_'+data_dict[b]+'.csv')\n",
    "    print datum1.shape\n",
    "    datum1.drop([u'Unnamed: 0'], axis=1, inplace=True)\n",
    "    datum1.drop_duplicates(inplace=True)\n",
    "    datum1.drop(0, inplace=True)\n",
    "    datum1.set_index(datum1.Fecha, inplace=True)\n",
    "    datum1 = datum1.Tweet\n",
    "    \n",
    "    datum2 = pd.read_csv('tweets_'+data_dict[b]+'sinceMarch.csv')\n",
    "    print datum2.shape\n",
    "    datum2.drop([u'Unnamed: 0'], axis=1, inplace=True)\n",
    "    datum2.drop_duplicates(inplace=True)\n",
    "    datum2.drop(0, inplace=True)\n",
    "    datum2.set_index(datum2.Fecha, inplace=True)\n",
    "    datum2 = datum2.Tweet\n",
    "    \n",
    "    data = pd.concat([datum1,datum2])\n",
    "    del datum1, datum2\n",
    "    print data.shape\n",
    "    data.drop_duplicates(inplace=True)\n",
    "    print data.shape\n",
    "    \n",
    "    data.drop_duplicates(inplace=True)\n",
    "    tokenized  = spanish_tweet_tokenizer_with_date(data)\n",
    "    #Creo el índice\n",
    "    ix_jan = tokenized.date_corrected.index>pd.to_datetime('2015-12-31 23:59:59')\n",
    "    #Creo la ts que satisface ese índice\n",
    "    jan_tweets = tokenized.date_corrected[ix_jan]\n",
    "    tokenized  = spanish_tweet_tokenizer_with_date(jan_tweets)\n",
    "    n_tweets = tokenized.n_tweets\n",
    "    #Proporción que dedican a cada cosa...\n",
    "    clt_frame  = tokenized.lookfor_intweets(clt_words)\n",
    "    for cat in mkt_categos_tags:\n",
    "        print cat\n",
    "        mkt_frame  = tokenized.lookfor_intweets(mkt_categos[cat])\n",
    "        mkt_tweets = mkt_frame.shape[0]\n",
    "        base = np.zeros((1,mkt_tweets))\n",
    "        print '-----', mkt_tweets, mkt_frame.shape\n",
    "        for word in clt_words:\n",
    "            temp = mkt_frame.str.contains(word).values.reshape(1,mkt_tweets)\n",
    "            base = np.concatenate((base,temp),axis=0)\n",
    "        ix_pure_mkt = pd.DataFrame(base[1:,:]).mean()==0\n",
    "        mkt_frame = mkt_frame[ix_pure_mkt.values]\n",
    "\n",
    "        mkt_num   = pd.Series([1.0 for i in mkt_frame],\n",
    "                              index=mkt_frame.index-pd.to_timedelta('6H'))\n",
    "\n",
    "        mkt_num.fillna(0,inplace=True)\n",
    "        print mkt_num.shape\n",
    "        try:\n",
    "            mkt_num.to_csv('mkt_frame_'+data_dict[b]+'_'+cat+'.csv')\n",
    "        except:\n",
    "            print 'No se pudo: ', mkt_num.sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "banamex\n",
      "(3201, 24)\n",
      "(3201, 18)\n",
      "(6400L,)\n",
      "(3601L,)\n",
      "daily avg mkt tweets 00:00 - <9:00:     0\n",
      "daily avg mkt tweets 9:00 - <18:00:     2.54621848739\n",
      "daily avg mkt tweets 18:00 - <00:00:    0.132743362832\n",
      "\n",
      "SantanderMx\n",
      "(3201, 24)\n",
      "(3201, 18)\n",
      "(6400L,)\n",
      "(6396L,)\n",
      "daily avg mkt tweets 00:00 - <9:00:     0.731707317073\n",
      "daily avg mkt tweets 9:00 - <18:00:     3.20325203252\n",
      "daily avg mkt tweets 18:00 - <00:00:    0.716666666667\n"
     ]
    }
   ],
   "source": [
    "# Hrs en que publican desde enero\n",
    "for b in xrange(n_accounts-2):\n",
    "    print '\\n',data_dict[b]\n",
    "    \n",
    "    datum1 = pd.read_csv('tweets_'+data_dict[b]+'.csv')\n",
    "    print datum1.shape\n",
    "    datum1.drop([u'Unnamed: 0'], axis=1, inplace=True)\n",
    "    datum1.drop_duplicates(inplace=True)\n",
    "    datum1.drop(0, inplace=True)\n",
    "    datum1.set_index(datum1.Fecha, inplace=True)\n",
    "    datum1 = datum1.Tweet\n",
    "    \n",
    "    datum2 = pd.read_csv('tweets_'+data_dict[b]+'sinceMarch.csv')\n",
    "    print datum2.shape\n",
    "    datum2.drop([u'Unnamed: 0'], axis=1, inplace=True)\n",
    "    datum2.drop_duplicates(inplace=True)\n",
    "    datum2.drop(0, inplace=True)\n",
    "    datum2.set_index(datum2.Fecha, inplace=True)\n",
    "    datum2 = datum2.Tweet\n",
    "    \n",
    "    data = pd.concat([datum1,datum2])\n",
    "    del datum1, datum2\n",
    "    print data.shape\n",
    "    data.drop_duplicates(inplace=True)\n",
    "    print data.shape\n",
    "    \n",
    "    tokenized  = spanish_tweet_tokenizer_with_date(data)\n",
    "    #Creo el índice\n",
    "    ix_jan = tokenized.date_corrected.index>pd.to_datetime('2015-12-31 23:59:59')\n",
    "    #Creo la ts que satisface ese índice\n",
    "    jan_tweets = tokenized.date_corrected[ix_jan]\n",
    "    tokenized  = spanish_tweet_tokenizer_with_date(jan_tweets)\n",
    "    n_tweets = tokenized.n_tweets\n",
    "    #Proporción que dedican a cada cosa...\n",
    "    clt_frame  = tokenized.lookfor_intweets(clt_words)\n",
    "    mkt_frame  = tokenized.lookfor_intweets(mkt_words)\n",
    "    mkt_tweets = mkt_frame.shape[0]\n",
    "    base = np.zeros((1,mkt_tweets))\n",
    "    for word in clt_words:\n",
    "        temp = mkt_frame.str.contains(word).values.reshape(1,mkt_tweets)\n",
    "        base = np.concatenate((base,temp),axis=0)\n",
    "    ix_pure_mkt = pd.DataFrame(base[1:,:]).mean()==0\n",
    "    mkt_frame = mkt_frame[ix_pure_mkt.values]\n",
    "\n",
    "    mkt_num   = pd.Series([1.0 for i in mkt_frame],\n",
    "                          index=mkt_frame.index-pd.to_timedelta('6H'))\n",
    "    \n",
    "    mkt_num.fillna(0,inplace=True)\n",
    "    mkt_num.to_csv('mkt_frame_'+data_dict[b]+'.csv')\n",
    "    \n",
    "    morn = mkt_num.between_time('00:00', '9:00',include_end=False).resample('1D',how='sum')\n",
    "    aft  = mkt_num.between_time('9:00', '18:00',include_end=False).resample('1D',how='sum')\n",
    "    noon = mkt_num.between_time('18:00','00:00',include_end=False).resample('1D',how='sum')\n",
    "    \n",
    "    morn.fillna(0,inplace=True)\n",
    "    aft.fillna(0,inplace=True)\n",
    "    noon.fillna(0,inplace=True)\n",
    "    \n",
    "    morn_avg = morn.mean()\n",
    "    aft_avg  = aft.mean()\n",
    "    noon_avg = noon.mean()\n",
    "    \n",
    "    if morn_avg>0:\n",
    "        0\n",
    "    else:\n",
    "        morn_avg = 0\n",
    "    print 'daily avg mkt tweets 00:00 - <9:00:    ',morn_avg\n",
    "    print 'daily avg mkt tweets 9:00 - <18:00:    ',aft_avg\n",
    "    print 'daily avg mkt tweets 18:00 - <00:00:   ',noon_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "banamex\n",
      "(3201, 24)\n",
      "(3201, 18)\n",
      "(6400L,)\n",
      "(3601L,)\n",
      "2014-09-07 11:00:05\n",
      "Captación\n",
      "(118L,)\n",
      "Crédito\n",
      "(351L,)\n",
      "Inversiones\n",
      "(91L,)\n",
      "TdC\n",
      "(603L,)\n",
      "Experiencia al cliente\n",
      "(29L,)\n",
      "Seguro\n",
      "(128L,)\n",
      "General\n",
      "(779L,)\n",
      "Viajes\n",
      "(79L,)\n",
      "Shows\n",
      "(355L,)\n",
      "Autos\n",
      "(62L,)\n",
      "Casa\n",
      "(41L,)\n",
      "Móvil o Internet\n",
      "(258L,)\n",
      "\n",
      "BBVABancomer\n",
      "(3200, 24)\n",
      "(3200, 18)\n",
      "(6398L,)\n",
      "(3614L,)\n",
      "2014-10-14 15:12:28\n",
      "Captación\n",
      "(152L,)\n",
      "Crédito\n",
      "(395L,)\n",
      "Inversiones\n",
      "(474L,)\n",
      "TdC\n",
      "(737L,)\n",
      "Experiencia al cliente\n",
      "(136L,)\n",
      "Seguro\n",
      "(289L,)\n",
      "General\n",
      "(924L,)\n",
      "Viajes\n",
      "(122L,)\n",
      "Shows\n",
      "(37L,)\n",
      "Autos\n",
      "(98L,)\n",
      "Casa\n",
      "(94L,)\n",
      "Móvil o Internet\n",
      "(513L,)\n"
     ]
    }
   ],
   "source": [
    "# Histórico bbva y banamex (por categoría y en general)\n",
    "for b in ['banamex','BBVABancomer']:\n",
    "    print '\\n',b\n",
    "    \n",
    "    datum1 = pd.read_csv('tweets_'+b+'.csv')\n",
    "    print datum1.shape\n",
    "    datum1.drop([u'Unnamed: 0'], axis=1, inplace=True)\n",
    "    datum1.drop_duplicates(inplace=True)\n",
    "    datum1.drop(0, inplace=True)\n",
    "    datum1.set_index(datum1.Fecha, inplace=True)\n",
    "    datum1 = datum1.Tweet\n",
    "    \n",
    "    datum2 = pd.read_csv('tweets_'+b+'sinceMarch.csv')\n",
    "    print datum2.shape\n",
    "    datum2.drop([u'Unnamed: 0'], axis=1, inplace=True)\n",
    "    datum2.drop_duplicates(inplace=True)\n",
    "    datum2.drop(0, inplace=True)\n",
    "    datum2.set_index(datum2.Fecha, inplace=True)\n",
    "    datum2 = datum2.Tweet\n",
    "    \n",
    "    data = pd.concat([datum1,datum2])\n",
    "    del datum1, datum2\n",
    "    print data.shape\n",
    "    data.drop_duplicates(inplace=True)\n",
    "    print data.shape\n",
    "    \n",
    "    \n",
    "    tokenized  = spanish_tweet_tokenizer_with_date(data)\n",
    "    clt_frame  = tokenized.lookfor_intweets(clt_words)\n",
    "    mkt_frame  = tokenized.lookfor_intweets(mkt_words)\n",
    "    mkt_tweets = mkt_frame.shape[0]\n",
    "    base = np.zeros((1,mkt_tweets))\n",
    "    for word in clt_words:\n",
    "        temp = mkt_frame.str.contains(word).values.reshape(1,mkt_tweets)\n",
    "        base = np.concatenate((base,temp),axis=0)\n",
    "    ix_pure_mkt = pd.DataFrame(base[1:,:]).mean()==0\n",
    "    mkt_frame = mkt_frame[ix_pure_mkt.values]\n",
    "\n",
    "    mkt_num   = pd.Series([1.0 for i in mkt_frame],\n",
    "                          index=mkt_frame.index-pd.to_timedelta('6H'))\n",
    "    print mkt_num.index.min()\n",
    "    mkt_num.fillna(0,inplace=True)\n",
    "    mkt_num.to_csv('mkt_frame_historic_'+b+'.csv')\n",
    "    for cat in mkt_categos_tags:\n",
    "        print cat\n",
    "        mkt_frame  = tokenized.lookfor_intweets(mkt_categos[cat])\n",
    "        mkt_tweets = mkt_frame.shape[0]\n",
    "        base = np.zeros((1,mkt_tweets))\n",
    "        for word in clt_words:\n",
    "            temp = mkt_frame.str.contains(word).values.reshape(1,mkt_tweets)\n",
    "            base = np.concatenate((base,temp),axis=0)\n",
    "        ix_pure_mkt = pd.DataFrame(base[1:,:]).mean()==0\n",
    "        mkt_frame = mkt_frame[ix_pure_mkt.values]\n",
    "\n",
    "        mkt_num   = pd.Series([1.0 for i in mkt_frame],\n",
    "                              index=mkt_frame.index-pd.to_timedelta('6H'))\n",
    "\n",
    "        mkt_num.fillna(0,inplace=True)\n",
    "        print mkt_num.shape\n",
    "        mkt_num.to_csv('mkt_frame_historic_'+b+'_'+cat+'.csv')"
   ]
  },
  
