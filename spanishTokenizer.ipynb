{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from nltk import *\n",
    "from stop_words import get_stop_words\n",
    "\n",
    "# import string\n",
    "from sklearn import feature_extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class spanish_tweet_tokenizer_with_date:\n",
    "    \"\"\"Recibe ts o df con dos columnas: fecha en formato twitter (como índice) y el texto de los tweets\"\"\"\n",
    "    \"\"\"Receives ts or df with two columns: date in twitter format (as index) and tweets text.\"\"\"\n",
    "    def __init__(self,tweets):\n",
    "        # Asumo que la información no tiene estorbos (nans, cosas distintas a fechas en el índice...)\n",
    "        # I assume info hasn't any hindrance\n",
    "        try:\n",
    "            tweets.stack()\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            date = pd.to_datetime([time.strftime('%Y-%m-%d %H:%M:%S', j) \n",
    "                                   for j in [time.strptime(i,'%a %b %d %H:%M:%S +0000 %Y') for i in tweets.index]])\n",
    "        except:\n",
    "            date = pd.to_datetime(tweets.index)\n",
    "        tokenizer = tokenize.simple.SpaceTokenizer() #para evitar problemas con acentuación\n",
    "        self.date_corrected = pd.Series(tweets.values,\n",
    "                                        index=date)\n",
    "        \n",
    "        tweets = tweets.str.lower()\n",
    "        tokens = []\n",
    "        dates  = []\n",
    "        tknd_tweets = [] #aquí almacenaré tweets ya tokenizados\n",
    "        try:\n",
    "            decoded_tweets = [x.decode('utf-8') for x in tweets.astype('string')]\n",
    "        except IndexError:\n",
    "            decoded_tweets = [x.decode('utf-8') for x in tweets.iloc[:,1].astype('string')]\n",
    "        for t in xrange(len(decoded_tweets)):\n",
    "            tokened = tokenizer.tokenize(decoded_tweets[t])\n",
    "            tknd_tweets.append(tokened)\n",
    "            tweet_date = date[t]\n",
    "            for word in xrange(len(tokened)):\n",
    "                dates.append(tweet_date)\n",
    "                if len(tokened[word])<=1:\n",
    "                    tokens.append(tokened[word])\n",
    "                elif tokened[word][-1] in [sign.decode('utf-8') for sign in ['?','!',',',':','.','-','\"',')']]:\n",
    "                    tokens.append(tokened[word][:-1])\n",
    "                elif tokened[word][0] in [sign.decode('utf-8') for sign in ['¿','¡',',',':','.','-','\"', '(']]:\n",
    "                    tokens.append(tokened[word][1:])\n",
    "                else:\n",
    "                    tokens.append(tokened[word].lower())\n",
    "        frame = pd.DataFrame(np.asarray(tokens).reshape(len(tokens)),\n",
    "                             columns=['tokens'],\n",
    "                             index=pd.to_datetime(dates))\n",
    "        self.tokened_tweets_df = frame\n",
    "        self.tokened_tweets_ts = frame.stack()\n",
    "        self.tokened_tweets = frame.values\n",
    "        self.n_tokens = frame.shape[0]\n",
    "        self.n_tweets = tweets.shape[0]\n",
    "        \n",
    "        # Agregaré en automático el cómputo sin stop_words\n",
    "        stop_words_sp = get_stop_words('spanish')\n",
    "        #algunas palabras que no incluye\n",
    "        stop_words_sp.append(' ')\n",
    "        stop_words_sp.append('')\n",
    "        stop_words_sp.append('-')\n",
    "        stop_words_sp.append(u'si')\n",
    "        stop_words_sp.append(u'sí')\n",
    "        ix_no_sw    = np.in1d(self.tokened_tweets, stop_words_sp)!=1 #stopwords\n",
    "        frame_no_sw = self.tokened_tweets_ts[ix_no_sw]\n",
    "\n",
    "        self.tokened_tweets_no_sw = frame_no_sw\n",
    "        self.n_tokens_no_sw       = frame_no_sw.shape[0]\n",
    "    \n",
    "    def n_tokens(self):\n",
    "        return self.n_words\n",
    "    \n",
    "    def n_tweets(self):\n",
    "        return self.n_tweets\n",
    "    \n",
    "    def lexical_diversity(self):\n",
    "        distinct_words = len(set([word for word in self.tokened_tweets_ts]))\n",
    "        total_words    = self.tokened_tweets.shape[0]\n",
    "        \n",
    "        lex_div = 1.*distinct_words/total_words\n",
    "        return lex_div\n",
    "    \n",
    "    def avg_words_p_tweet(self):\n",
    "        avg_words = 1.*self.n_tokens/self.n_tweets\n",
    "        return avg_words\n",
    "    \n",
    "    def tokened_tweets_no_sw(self):\n",
    "        return self.tokened_tweets_no_sw\n",
    "        \n",
    "    def n_tokens_no_sw(self):\n",
    "        return self.n_tokens_no_sw\n",
    "    \n",
    "    def lexical_diversity_no_sw(self):\n",
    "        distinct_words = len(set([word for word in self.tokened_tweets_no_sw]))\n",
    "        total_words    = self.n_tokens_no_sw\n",
    "\n",
    "        lex_div = 1.*distinct_words/total_words\n",
    "        return lex_div\n",
    "\n",
    "    def avg_words_p_tweet_no_sw(self):\n",
    "        avg_words = 1.*self.n_tokens_no_sw/self.n_tweets\n",
    "        return avg_words\n",
    "    \n",
    "    def lookfor(self, words_to_look, include_sw):\n",
    "        \"\"\"Returns a filtered ts that includes only words_to_look, over ts with or without stopwords\"\"\"\n",
    "        if include_sw==True:\n",
    "            frame = self.tokened_tweets_no_sw\n",
    "            n_obs = self.n_tokens_no_sw\n",
    "        else:\n",
    "            frame = self.tokened_tweets_ts\n",
    "            n_obs = self.n_tokens\n",
    "        \n",
    "        temp_ix = np.ones((1,n_obs))\n",
    "        \n",
    "        for w in words_to_look:\n",
    "            temp_ix = np.concatenate((temp_ix,frame.str.match(w).values.reshape(1,n_obs)),\n",
    "                                     axis=0)\n",
    "        ix    = pd.DataFrame(temp_ix[1:,:])\n",
    "        \n",
    "        final = frame.loc[(ix.mean()>0).values]\n",
    "        \n",
    "        return final\n",
    "    def lookfor_intweets(self, words_to_look):\n",
    "        \"\"\"Retrieves tweets that contain any of the given words.\"\"\"\n",
    "        frame = self.date_corrected.str.lower()\n",
    "        n_obs = frame.shape[0]\n",
    "        \n",
    "        temp_ix = np.ones((1,n_obs))\n",
    "        \n",
    "        for w in words_to_look:\n",
    "            temp_ix = np.concatenate((temp_ix,frame.str.contains(w).values.reshape(1,n_obs)),\n",
    "                                     axis=0)\n",
    "        ix    = pd.DataFrame(temp_ix[1:,:])\n",
    "        final = frame.loc[(ix.sum()>0).values]\n",
    "        \n",
    "        return final"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
